apiVersion: "nais.io/v1"
kind: "Alert"
metadata:
  name: flex-alerts-gcp
  namespace: flex
  labels:
    team: flex
spec:
  receivers:
    slack:
      channel: '#spøkelser'
      #prependText: '<!here> | '
  alerts:
    - alert: flex-app-nede
      expr: sum(up{team="flex"}) by (app) == 0
      for: 1m
      description: "{{ $labels.app }} er nede"
      action: "Se `kubectl describe pod {{ $labels.pod_name }} -n flex` for events, og `kubectl logs {{ $labels.pod_name }} -c {{ $labels.app }} -n flex` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: flex-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{namespace="flex", container!="elector", container!="cloudsql-proxy", container!="linkerd-proxy"}[30m])) by (container) > 2
      for: 1m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.pod_name }} -n flex` for events, og `kubectl logs {{ $labels.pod_name }} -c {{ $labels.app }} -n flex` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: flex-not-ready
      expr: sum(kube_pod_container_status_ready{namespace="flex", container!="elector", container!="cloudsql-proxy", container!="linkerd-proxy"} == 0) by (container) > 0
      for: 10m
      description: "{{ $labels.container }} svarer ikke ok på readiness de siste 10 minutter"
      action: "Se `kubectl describe pod {{ $labels.pod_name }} -n flex` for events, og `kubectl logs {{ $labels.pod_name }} -c {{ $labels.app }} -n flex` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: flex-errorlogging
      expr: (100 * sum by (log_app) (rate(logd_messages_total{log_namespace="flex",log_level="Error"}[5m])) / sum by (log_app) (rate(logd_messages_total{log_namespace="flex"}[5m]))) > 10
      for: 3m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk loggene til {{ $labels.log_app }}, for å se hvorfor det er så mye feil (over 10 feil per 100 logger de siste 5 minuttene)"
    - alert: flex-mangler-metrikker
      expr: count(up{team="flex"} offset 1h) by (app) unless count(up{team="flex"}) by (app)
      for: 5m
      description: "{{ $labels.app }} har sluttet å rapporterer metrikker"
      action: "Sjekk om {{ $labels.app }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: høyt antall HTTP server errors (500 responses)
      severity: danger
      expr: (100 * (sum by (app, namespace) (rate(response_total{namespace="flex",status_code="500",app!~"linkerd-proxy"}[3m]))/sum by (app, namespace) (rate(response_total{namespace="flex",app!~"linkerd-proxy"}[3m])))) > 1
      for: 3m
      action: "Sjekk loggene for å se grunnen til at {{ $labels.app }} returnerer feilmeldinger."
    - alert: flex-app-cpu
      expr: (sum(kube_pod_container_resource_requests{namespace="flex",resource="cpu", container!="linkerd-proxy",container!="cloudsql-proxy"}) by (container) - sum(rate(container_cpu_usage_seconds_total{namespace="flex",container!="linkerd-proxy",container!="cloudsql-proxy",container!="POD", container!=""}[5m])) by (container)) < 0
      for: 5m
      description: "{{ $labels.container }} bruker mer CPU enn angitt"
      action: "Sjekk CPU for {{ $labels.container }}, kanskje juster opp requested eller justere opp antall pods"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: flex-app-ram
      expr: sum(kube_pod_container_resource_requests{namespace="flex", resource="memory", container!="linkerd-proxy",container!="cloudsql-proxy"}) by (pod) - sum(container_memory_working_set_bytes{namespace="flex",container!="linkerd-init",container!="linkerd-proxy",container!="cloudsql-proxy",container!="POD", container!=""}) by (pod) < 0
      for: 5m
      description: "{{ $labels.container }} bruker mer RAM enn angitt"
      action: "Sjekk RAM for {{ $labels.container }}, kanskje juster opp requested eller justere opp antall pods"
      sla: respond within 1h, during office hours
      severity: danger
